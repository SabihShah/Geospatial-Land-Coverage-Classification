{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Implement and Test a PyTorch-Based Classifier</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Objective</h2><ul>\n",
    "After completing this lab, you'll be able to:\n",
    "\n",
    "1. Create a PyTorch-based CNN model for classification.\n",
    "2. Train this model for the classification of agricultural and non-agricultural land.\n",
    "3. Evaluate the performance of this CNN model.\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of building, training, and evaluating a **PyTorch-based Convolutional Neural Network (CNN)** for image classification, for agricultural images in your case. You will cover the following:\n",
    "1. *Data preparation*\n",
    "2. *Model architecture* definition\n",
    "3. *Training*, and\n",
    "4.  Model *performance analysis*\n",
    "\n",
    "The goal is to classify satellite images into two categories: 'agricultural' and 'non-agricultural'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "\n",
    "1. [Configuration and library imports](#Configuration-and-library-imports)\n",
    "2. [Data acquisition and preparation](#Data-acquisition-and-preparation)\n",
    "3. [Ensuring repeatability in PyTorch](#Ensuring-repeatability-in-PyTorch)\n",
    "4. [Defining hyperparameters and device](#Defining-hyperparameters-and-device)\n",
    "5. [The data pipeline](#The-data-pipeline)\n",
    "6. [Defining the model](#Defining-the-model)\n",
    "7. [Training and validation](#Training-and-validation)\n",
    "8. [Save and download the trained model weights](#Save-and-download-the-trained-model-weights)\n",
    "9. [Visualizing training history](#Visualizing-training-history)\n",
    "10. [Final model evaluation](#Final-model-evaluation)\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and library imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them, it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where you try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"Library installation was successful, let's proceed ahead\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library installation - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 ms, sys: 14.6 ms, total: 40.2 ms\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if the above libraries installed properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library installation was successful, let's proceed ahead\n"
     ]
    }
   ],
   "source": [
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the `PyTorch` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 15.5 ms, sys: 9.9 ms, total: 25.4 ms\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torch==2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torchvision` library installation\n",
    "\n",
    "Install the `torchvision` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision==0.22 in /opt/conda/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (1.26.0)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision==0.22) (11.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (75.8.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.7.0->torchvision==0.22) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision==0.22) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.7.0->torchvision==0.22) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 18.3 ms, sys: 7.97 ms, total: 26.3 ms\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install torchvision==0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit-learn` library installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.7.0 in /opt/conda/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 17.2 ms, sys: 9.49 ms, total: 26.7 ms\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skillsnetwork\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries\n",
      "CPU times: user 3.63 s, sys: 673 ms, total: 4.3 s\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- AI LIBRARY IMPORTS ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Imported libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Data Extraction Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = \".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition and preparation\n",
    "\n",
    "### Defining dataset URL\n",
    "\n",
    "\n",
    "Let's define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (like S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "1. Download and extract data from the cloud using the `skillsnetwork.prepare` method\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats like gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e003afc1719b4fe79e1aebd573e5384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faba35b0a46a456a985b5ab91e01a6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name# Get the filename from the URL (e.g., 'data.tar')\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring repeatability in PyTorch\n",
    "\n",
    "To achieve reproducible results when you train a CNN in PyTorch, you must follow three steps:\n",
    "\n",
    "1.  Define a helper called `set_seed` that seeds every random-number generator and configures cuDNN for deterministic kernels.\n",
    "2.  Call `set_seed()` *once* at the top of your script/notebook to lock in the seed for the main process.\n",
    "3.  Provide a `worker_init_fn` so each `DataLoader` worker starts from a reproducible seed as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the `set_seed` function\n",
    "What the `set_seed` function does\n",
    "\n",
    "* **Python & NumPy** – Many data-pipeline utilities (shuffling lists, image augmentations) rely on these random-number generators. Seeding them first removes one entire layer of randomness.\n",
    "* **PyTorch CPU / GPU** – `torch.manual_seed` covers every op executed on the CPU, while `torch.cuda.manual_seed_all` applies the same seed to each GPU stream so that multi-GPU jobs stay in sync.\n",
    "* **cuDNN flags** – By default cuDNN picks the fastest convolution algorithm, which can vary run-to-run. Setting `deterministic=True` forces repeatable kernels and turning `benchmark` *off* prevents the auto-tuner from replacing those kernels mid-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch (CPU & all GPUs) and\n",
    "    make cuDNN run in deterministic mode.\"\"\"\n",
    "    # ---- Python and NumPy -------------------------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n",
    "    torch.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "    # ---- cuDNN: force repeatable convolutions -----------------------\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark     = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `set_seed()`\n",
    "\n",
    "Running the command *before* you build models, create datasets, or start data-loader workers guarantees that every downstream object inherits the same seed.  If you call it later, some layers or tensors may already have been initialised with non-deterministic values, breaking repeatability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 42 — main process is now deterministic.\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Global seed set to {SEED} — main process is now deterministic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now know how to fix the seed for reproducibility. Now, let's answer the following question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why is random initialization useful for the model? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n",
    "# Random initialization, each neuron starts differently, enabling effective learning and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "Random initialization, each neuron starts differently, enabling effective learning and convergence.    \n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `worker_init_fn` function\n",
    "\n",
    "PyTorch offsets each worker’s seed by default, injecting new randomness. For reproducible results, you want workers to start from **fixed** seeds so every data-augmentation decision (flip, crop, colour-jitter) is repeatable across runs. The `worker_init_fn` function re-seeds Python, NumPy, and PyTorch CPU random-number generators inside **each** worker using a simple deterministic formula (`SEED + worker_id`).  The result will be identical batches, identical gradients, and identical model checkpoints, run after run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id: int) -> None:\n",
    "    \"\"\"Re-seed each DataLoader worker so their RNGs don't collide.\"\"\"\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed) \n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining `dataset_path`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(extract_dir, \"images_dataSAT\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters and device\n",
    "\n",
    "You have to define the key **hyperparameters** that control the model's training process. Hyperparameters are set by the user to configure the learning algorithm.\n",
    "\n",
    "- **`img_size`**: The spatial resolution (height and width) to which all images will be resized. This ensures that the input to the neural network is of a consistent size.\n",
    "- **`batch_size`**: The number of training examples utilized in one iteration (one forward and backward pass). A larger batch size can lead to faster training but requires more memory.\n",
    "- **`lr` (Learning Rate)**: A crucial hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.\n",
    "- **`epochs`**: The number of times the learning algorithm will work through the entire training dataset.\n",
    "- **`model_name`**: The name of the model file that will be created after training. This is useful for saving the checkpoint while training.\n",
    "- **`device`**: This line programmatically checks if a CUDA-enabled GPU is available using `torch.cuda.is_available()`. If a GPU is found, the device is set to `\"cuda\"` to leverage hardware acceleration. Otherwise, it defaults to the `\"cpu\"`. This makes the code portable and efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used is cpu\n"
     ]
    }
   ],
   "source": [
    "img_size = 64\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epochs = 3 # set to low number for your convenience. You can change this to any number of your liking\n",
    "model_name = \"ai_capstone_pytorch_state_dict.pth\"\n",
    "num_classes = 2 #number of classes in the dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device used is {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data pipeline\n",
    "You have downloaded the dataset and fixed the initial random seed for reproducibility. Now, you can start to build the data pipeline to feed data for training the model.\n",
    "To create the data pipeline for PyTorch, you will:\n",
    "1. Define transformations\n",
    "2. Split the dataset for training and validation\n",
    "3. Create the dataloader to feed the data into the training model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define transformations\n",
    "Here, you will define a sequence of operations to be applied to the training images. It includes **data augmentation** techniques like `RandomRotation`, `RandomHorizontalFlip`, and `RandomAffine`. Augmentation artificially expands the training dataset by creating modified versions of images, which helps the model generalize better and reduces overfitting. The pipeline also resizes the image, converts it to a PyTorch tensor, and normalizes its pixel values.\n",
    "This cell constructs the entire pipeline for loading and preparing the image data for the model. It involves defining transformations, splitting the data, and creating data loaders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create the training transformation pipeline `train_transform` using the `tranforms.Compose` . \n",
    "You may use \n",
    "- `transforms.Resize` : To resize all input images to a fixed size, useful for input vector with fixed dimensions for model training\n",
    "- `transforms.RandomRotation`: For geometrical rotation\n",
    "- `transforms.RandomHorizontalFlip`: For Geometrical horizontal flipping\n",
    "- `transforms.RandomAffine`: For adjusting to a different point-of-view\n",
    "\n",
    "Then, convert the image array to a Tensor using `transforms.ToTensor()`.\n",
    "\n",
    "And finally, normalize the images between [-1,1] using `transforms.Normalize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(0, shear=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "train_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                      transforms.RandomRotation(40),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomAffine(0, shear=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`train_transform`**: This defines a sequence of operations to be applied to the training images. It includes **data augmentation** techniques like `RandomRotation`, `RandomHorizontalFlip`, and `RandomAffine`. Augmentation artificially expands the training dataset by creating modified versions of images, which helps the model generalize better and reduces overfitting. The pipeline also resizes the image, converts it to a PyTorch tensor, and normalizes its pixel values.\n",
    "- **`val_transform`**: The transformation for the validation set is simpler. It omits the random augmentation steps because you want to evaluate the model's performance on the original, unaltered data.\n",
    "- **`datasets.ImageFolder`**: This PyTorch utility automatically loads an image dataset from a directory where subdirectories are named after their corresponding classes (e.g., `data/agri`, `data/non_agri`).\n",
    "- **`random_split`**: The full dataset is partitioned into training (80%) and validation (20%) sets. This separation is crucial for assessing how well the model generalizes to unseen data.\n",
    "- **`DataLoader`**: These objects wrap the datasets and provide an efficient, iterable way to feed data to the model in batches. `shuffle=True` for the `train_loader` ensures that the model sees the data in a different order each epoch, which helps prevent it from learning the order of the training examples. `worker_init_fn` ensures that **fixed seed** is passed to the `dataloader` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create the validation transformation pipeline `val_transform`.\n",
    "The validataion dataset is just for validating the preformace of the model and hence, doesn't need to augment the input images. \n",
    "So, you may use \n",
    "- `transforms.Resize` : To resize all input images to a fixed size\n",
    "- `transforms.ToTensor()`\n",
    "-  `transforms.Normalize`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "val_transform = transforms.Compose([\n",
    "                                    transforms.Resize((img_size, img_size)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "You have defined the transformation pipelines for the training and validation datasets. \n",
    "\n",
    "Next, you will use the `datasets.ImageFolder` utility to load an image dataset from the root directory `dataset_path`. \n",
    "\n",
    "This root directory contains the subdirectories where each subdirectory corresponds to a class (e.g., `data/agri`, `data/non_agri`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset split: Train and validation\n",
    "\n",
    "The next step in the data loading pipeline is to split the image dataset for training and validation. \n",
    "\n",
    "You can use `random_split` from `torch.utils.data` class. \n",
    "\n",
    "This method allows you to randomly split the input data based on a pre-defined split ratio for the training and validation datasets. \n",
    "\n",
    "In this case, you can use 80% (0.8) dataset for training and 20% (0.20) for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training dataLoader\n",
    "\n",
    "Now, you can use the `DataLoader` from `torch.utils.data` class to create a dataset generator for lazy loading of the training dataset.\n",
    "In the input, you define \n",
    "- `train_dataset`: The training image dataset\n",
    "- `batch_size`: The number of images to be loaded in each batch\n",
    "- `shuffle`: Set to *True* to load images from the dataset in random order\n",
    "- `num_workers`: Number of parallel processes used to load the images. This is for optimum utilization of your CPU cores to reduce the image I/O bottleneck\n",
    "- `worker_init_fn`: For function to decide on data augmentation. The default is with *random seed* for better generalization or *fixed seed* for reproducible results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          worker_init_fn=worker_init_fn\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation DataLoader\n",
    " \n",
    "Now that you know how to create the train dataloader, in this step, you will create a validation step dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: create `val_loader` for the validation dataset\n",
    "\n",
    "You have to create the validation dataloader `val_loader` for validation of model in each training step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "validation_loader = DataLoader(val_dataset,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True,\n",
    "                               num_workers=4,\n",
    "                               worker_init_fn=worker_init_fn\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=4,\n",
    "                        worker_init_fn=worker_init_fn\n",
    "                       )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Dataloaders. Now creating the model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Created Dataloaders. Now creating the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "You will define the Convolutional Neural Network (CNN) architecture and configure the components needed for training.\n",
    "<p></p>\n",
    "\n",
    "\n",
    "<p></p>\n",
    "\n",
    "- **`model = nn.Sequential(...)`**: A sequential container is used to build the model as a linear stack of layers. This is a convenient way to define a straightforward CNN.\n",
    "  - **Convolutional Blocks**: The model consists of several blocks, each containing\n",
    "      - a `Conv2d` layer for feature extraction,\n",
    "      - a `ReLU` activation function,\n",
    "      - a `MaxPool2d` layer to downsample and reduce dimensionality,\n",
    "      - a`BatchNorm2d` to stabilize and accelerate training.    \n",
    "  - **Classifier**: After the convolutional blocks,\n",
    "      - `AdaptiveAvgPool2d` reduces each feature map to a single value, making the model more robust to input size variations.\n",
    "      - `Flatten` converts the 2D feature maps into a 1D vector.\n",
    "      - `Linear` (fully connected) layers then perform the final classification,\n",
    "      - `Dropout` is used as a regularization technique to prevent overfitting.\n",
    "  - **`.to(device)`**: This moves the model's parameters and buffers to the selected device (GPU, if available otherwise CPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL ---\n",
    "model = nn.Sequential(\n",
    "                        # Conv Block 1\n",
    "                        nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(),\n",
    "                        nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "                        \n",
    "                        # Conv Block 2-6\n",
    "                        nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "                        nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "                        nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "                        nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "                        nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "                        \n",
    "                        # Classifier\n",
    "                        nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "                        nn.Linear(1024, 2048), nn.ReLU(), nn.BatchNorm1d(2048), nn.Dropout(0.4),\n",
    "                        nn.Linear(2048, num_classes)\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training setup\n",
    "\n",
    "After defining the model, you declare the loss function and the optimizer for backpropagation and learning\n",
    "You also set up the tracking of the history of the model training for loss and accuracy for every step of the model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The loss function is defined using **`criterion = nn.CrossEntropyLoss()`**\n",
    "    - `CrossEntropyLoss` is specifically designed for multi classs classification problems.\n",
    "<p></p>\n",
    "<p></p>\n",
    "- The optimizer is defined using **`optimizer = optim.Adam(...)`**:\n",
    "    - The Adam optimizer is chosen to update the model's weights. It's an adaptive learning rate method that is computationally efficient and works well in practice.\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- You can **track the history** using `best_loss`, `loss_history` and `acc_history` dictionaries\n",
    "    - `best_loss`: stores the best validation loss achieved so far.\n",
    "    - `loss_history` and `acc_history` dictionaries to log the loss and accuracy history for plotting later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Model. Now training the model...\n"
     ]
    }
   ],
   "source": [
    "# --- TRAINING SETUP ---\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "best_loss = float('inf')\n",
    "loss_history = {'train': [], 'val': []}\n",
    "acc_history = {'train': [], 'val': []}\n",
    "\n",
    "print(\"Created Model. Now training the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation\n",
    "\n",
    "Your neural network is now ready for training.\n",
    "\n",
    "Here, you will set up the main logic for how the model learns from the data. The model iterates through the dataset for the specified number of epochs, with each epoch consisting of a training phase and a validation phase.\n",
    "\n",
    "- **Outer Loop (`for epoch in range(epochs)`)**: Controls the number of full passes over the dataset.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "\n",
    "- **In the interest of time, we are training the model for just 3 epochs**.\n",
    "    - Generally, you train a model for many more epochs (atleast 15, usually). The model trained for 20 epochs can be found **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Training Phase**:\n",
    "  - `model.train()`: Sets the model to training mode. This activates layers like Dropout and ensures BatchNorm layers learn from the current batch statistics.\n",
    "  - **Inner Loop (`for images, labels in train_loader`)**: Iterates over batches of training data.\n",
    "  - `optimizer.zero_grad()`: Clears the gradients from the previous iteration before computing new ones.\n",
    "  - `outputs = model(images)`: **Forward Pass**. The input data is passed through the network to get predictions (logits).\n",
    "  - `loss.backward()`: **Backward Pass**. Gradients of the loss with respect to the model's parameters are calculated.\n",
    "  - `optimizer.step()`: The optimizer updates the model's parameters using the computed gradients.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Validation Phase**:\n",
    "  - `model.eval()`: Sets the model to evaluation mode. This deactivates Dropout and makes BatchNorm layers use their learned running statistics.\n",
    "  - `with torch.no_grad()`: Disables gradient calculation, which speeds up computation and reduces memory usage since you are only evaluating, not training.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "- **Model Checkpointing**: After each epoch, the current validation loss is compared to the `best_loss` seen so far. If the current loss is lower, the model's state (`model.state_dict()`) is saved to a file. This ensures that you keep the model version that performed best on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on : ===cpu=== with batch size: 128 & lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   3%|▎         | 1/38 [00:07<04:47,  7.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# labels as integer class indices\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# outputs are raw logits\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# criterion is nn.CrossEntropyLoss\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Training on : ==={device}=== with batch size: {batch_size} & lr: {lr}\")\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    start_time = time.time() # to get the training time for each epoch\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0  # for the training metrics\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "        images, labels = images.to(device), labels.to(device)  # labels as integer class indices\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # outputs are raw logits\n",
    "        loss = criterion(outputs, labels)  # criterion is nn.CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "  \n",
    "    # Synchronize CUDA before stopping timer (if using GPU)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0 #  for the validation metrics\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "  \n",
    "    # Save the best model\n",
    "    avg_val_loss = val_loss/len(val_loader)\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "    \n",
    "    # Store metrics\n",
    "    loss_history['train'].append(train_loss/len(train_loader))\n",
    "    loss_history['val'].append(val_loss/len(val_loader))\n",
    "    acc_history['train'].append(train_correct/train_total)\n",
    "    acc_history['val'].append(val_correct/val_total)\n",
    "    \n",
    "    #print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {loss_history['train'][-1]:.4f} | Val Loss: {loss_history['val'][-1]:.4f}\")\n",
    "    print(f\"Train Acc: {acc_history['train'][-1]:.4f} | Val Acc: {acc_history['val'][-1]:.4f}\")\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1} training completed in {epoch_time:.2f} seconds\\n\") \n",
    "\n",
    "print(\"Trained Model. Now evaluating the model...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully trained the model using PyTorch libraries. As you can see, during the model training, each step is easy accessible for fine tuning the model. This gives the user an advantage by giving them control over every hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training cell above, please answer the following questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What is `tqdm` used for?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use this cell to type the answer to the question.\n",
    "\n",
    "# tqdm is used for visualizing progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"The tqdm library is used to provide a progress bar to monitor the progress of each epoch.\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why are the `train_loss`, `train_correct` and `train_total` set to 0 in every epoch?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n",
    "# Because they accumulate metrics for that specific epoch only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"Because they accumulate metrics for that specific epoch only\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why do you need to use `torch.no_grad()` in the validation loop?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n",
    "# It disables gradient calculation as you do not need gradient calculation for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"It disables gradient calculation as you do not need gradient calculation for validation\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What are two different metrics on which the model can be evaluated for best performance during training?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n",
    "# the validation loss and validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\"the validation loss and validation accuracy\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and download the trained model weights\n",
    "\n",
    "For your convenience, I have saved a model state dict for the model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth)**.\n",
    "\n",
    "\n",
    "Otherwise, you have also saved the model state dictionary for the best model using the `torch.save` function during training in this lab.\n",
    "\n",
    "You can also download the model state dict for the model that you have just trained for use in the subsequent labs.\n",
    "\n",
    "This is the PyTorch AI model state that can now be used for infering un-classified images. \n",
    "\n",
    "- You can download the trained model file: `ai_capstone_pytorch_state_dict.pth` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clicking \"Download\".\n",
    "- This model could be used in other labs of this AI capstone course, instead of the model provided at the above link\n",
    "\n",
    "\n",
    "Please refer to the screenshots below for downloading the file to your local computer.\n",
    "\n",
    "\n",
    "### The trained model state file (`ai_capstone_pytorch_state_dict.pth` ) in the left pane\n",
    "![Model_PyTorch_download_screenshot_1_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Nar8kA3Qrz3uCmFtFrKI9g/Model-PyTorch-download-screenshot-1-marked.png)\n",
    "\n",
    "### The **download** option\n",
    "![Model_PyTorch_download_screenshot_2_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HKO5ROsE1erbqcE6Kq8ysA/Model-PyTorch-download-screenshot-2-marked.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training history\n",
    "\n",
    "Here, you can use `matplotlib` to create plots of the model's accuracy and loss over each epoch. Visualizing these metrics is useful for understanding the training dynamics.\n",
    "\n",
    "\n",
    "Usually, the following two plots are used to track the training history of a model:\n",
    "- **Accuracy Plot**: Shows the training accuracy versus the validation accuracy. A large gap between the two curves can be an indicator of overfitting, where the model performs well on the data it has seen but poorly on new, unseen data.\n",
    "\n",
    "- **Loss Plot**: Shows the training loss versus the validation loss. An ideal plot shows both losses decreasing and converging. If the validation loss starts to increase while the training loss continues to decrease, it's a strong sign of overfitting.\n",
    "\n",
    " \n",
    "These plots provide an intuitive, visual summary of the entire training process and help diagnose potential issues or confirm that the model has trained successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the **Model Accuracy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(acc_history['train'], label='Train Acc')\n",
    "plt.plot(acc_history['val'], label='Val Acc')\n",
    "plt.title('Model Accuracy (PyTorch)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write the code for the loss plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Plot the *Model Loss* from the training history of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(loss_history['train'], label='Train Loss')\n",
    "plt.plot(loss_history['val'], label='Val Loss')\n",
    "plt.title('Model Loss (PyTorch)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model evaluation\n",
    "\n",
    "This cell comprehensively evaluates the best-performing model saved during the training loop. While accuracy provides a high-level view, these metrics provide a deeper insight into the model's behavior.\n",
    "\n",
    "\n",
    "- **`model.eval()`**: Switches the model to evaluation mode.\n",
    "- **`with torch.no_grad()`**: Disables gradient computation for efficiency.\n",
    "- **Collecting Predictions**: The code iterates through the entire validation set to gather all predictions and their corresponding true labels.\n",
    "\n",
    "- **`accuracy`**: The proportion of correct predictions out of the total predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, you have to get the predictions for the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: For the images from `val_loader`, get a list of :\n",
    "<p></p>\n",
    "\n",
    "<p></p>\n",
    " \n",
    "**1.** all predictions `all_preds`\n",
    " \n",
    "**2.** the ground truth labels `all_labels` \n",
    "\n",
    "For the predictions, you will have to move the data to the CPU using `predictions.cpu()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the answer to the question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels.extend(labels.numpy())\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy of the plot\n",
    "\n",
    "**`accuracy`** is the proportion of correct predictions out of the total predictions.\n",
    "\n",
    "You can use `accuracy_score` from `sklearn.metrics` class to calculate the **accuracy_score**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"The accuracy of the model is: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulation! You've successfully built, trained, and evaluated a deep learning model for image classification using PyTorch.\n",
    "\n",
    "In this lab, you coded for:\n",
    "- **Data loading pipeline:** Implementing ImageDataGenerator for efficient on-the-fly image loading, resizing, normalization, and vital data augmentation.\n",
    "- **CNN architecture:** Building a multi-layered CNN incorporating Conv2D layers.\n",
    "- **Model training setup:** Configuring the model’s learning process using the Adam optimizer, BCEWithLogitsLoss, and tracking accuracy and loss metrics.\n",
    "- **Training process:** Executing the training loop by feeding data in batches and monitoring performance over epochs.\n",
    "- **Performance visualization:** Plotting accuracy and loss curves to analyze learning progress and detect overfitting.\n",
    "- **Model evaluation:** Assessing model performance using accuracy_score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-04  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "0438099ac7cecf8186ed65f816edd2dc3553a779fcf8536358210af38290d216"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
